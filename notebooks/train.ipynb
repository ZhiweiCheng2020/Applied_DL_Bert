{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "curr_path = os.getcwd()\n",
    "print(curr_path)\n",
    "sys.path.insert(0, os.path.dirname(curr_path))\n",
    "\n",
    "import src.data.data_preprocess as preprocess\n",
    "from src.models import bert\n",
    "from src.data import data_preprocess\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "torch.random.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_all = 13478 # the length of the whole dataset\n",
    "len_all = 1000 # the length of the whole dataset\n",
    "\n",
    "# split the data to (train & validation) and test sets\n",
    "test_set_size = int(len_all * 0.15)\n",
    "random.seed(1)\n",
    "test_idx = random.sample(range(0, len_all), test_set_size)\n",
    "train_val_idx = list(set(list(range(len_all))) - set(test_idx))\n",
    "\n",
    "test_dataset = preprocess.seq_dataset(data_path=os.path.join(os.path.dirname(curr_path), \n",
    "                                            \"data/imgt_I_domain_1_2.csv\"), seqs_range = test_idx)\n",
    "\n",
    "train_val_dataset = preprocess.seq_dataset(data_path=os.path.join(os.path.dirname(curr_path), \n",
    "                                            \"data/imgt_I_domain_1_2.csv\"), seqs_range = train_val_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_dataset[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "ebd_dim = 60 # Embedding Size\n",
    "vocab_size = 24\n",
    "max_seq_len = 182 # maximum sequence length is 181, but we add a [CLS] before it.\n",
    "num_layer = 2 # number of Encoder of Encoder Layer\n",
    "num_head = 2 # number of heads in Multi-Head Attention\n",
    "dim_feedforward = ebd_dim * 4  # 4*ebd_dim, FeedForward dimension\n",
    "\n",
    "# training parameters\n",
    "lr=0.0002\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_seqs = len(train_val_dataset)\n",
    "num_iters = math.ceil(num_seqs/batch_size)\n",
    "print(f'num_seqs: {num_seqs}, num_iters: {num_iters}')\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV in pytorch\n",
    "# https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\n",
    "# https://stackoverflow.com/questions/63224426/how-can-i-cross-validate-by-pytorch-and-optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "min_val_loss = np.inf # to track the minimal validation loss\n",
    "\n",
    "Bert_model = bert.Bert(ebd_dim=ebd_dim, num_head=num_head, vocab_size=vocab_size,\n",
    "                        dim_feedforward=dim_feedforward,\n",
    "                        num_layer=num_layer, max_seq_len=max_seq_len)\n",
    "\n",
    "train_val_dataset = preprocess.seq_dataset(data_path=os.path.join(os.path.dirname(curr_path), \n",
    "                                            \"data/imgt_I_domain_1_2.csv\"), seqs_range = train_val_idx,\n",
    "                                          seed=42)\n",
    "\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, \n",
    "                                            [train_size, val_size],\n",
    "                                # generator=torch.Generator().manual_seed(42),\n",
    "                                )\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss()\n",
    "    # Loss_CE = nn.CrossEntropyLoss()\n",
    "    # Loss_BCE = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(Bert_model.parameters(), lr=lr)\n",
    "    \n",
    "    # train_loader, is_train, optimizer, Bert_model, Loss\n",
    "    for i, (token_data, mask_pos_data, mask_token_data, code0, code1, code2, padding_mask) in enumerate(train_loader):\n",
    "        # remove the extra dimension\n",
    "        token_data, mask_pos_data, mask_token_data, padding_mask, code0, code1, code2 = token_data.squeeze(1), \\\n",
    "        mask_pos_data.squeeze(1), mask_token_data.squeeze(1), padding_mask.squeeze(1),\\\n",
    "            code0.squeeze(1), code1.squeeze(1), code2.squeeze(1)\n",
    "\n",
    "        # train the model\n",
    "        optimizer.zero_grad()\n",
    "        MaskedLM, code0_pred, code1_pred, code2_pred = Bert_model(input=token_data, padding_mask=padding_mask)\n",
    "        # masked token prediction\n",
    "        # (batch_size, n_mask) --> (batch_size,n_mask,vocab_size)\n",
    "        masked_pos = mask_pos_data.unsqueeze(-1).expand(-1, -1, MaskedLM.size(-1))\n",
    "        # (batch_size,source_len,vocab_size) --> (batch_size,n_mask,vocab_size)\n",
    "        MaskedLM = torch.gather(MaskedLM, 1, masked_pos)\n",
    "        # calculate the training loss       \n",
    "        loss_maskLM = Loss(MaskedLM.transpose(1,2), mask_token_data)\n",
    "        \n",
    "        loss_code0 = Loss(code0_pred, code0.float())\n",
    "        \n",
    "        curr_loss = loss_maskLM + loss_code0\n",
    "        \n",
    "        curr_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += curr_loss.item()\n",
    "\n",
    "        # # protein type prediction\n",
    "        # if ((i+1) % 10 == 0) and verbose:\n",
    "        #     print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{num_iters}, training cost = {curr_loss.item():.5f}')\n",
    "        \n",
    "        \n",
    "    epoch_train_loss = train_loss/len(train_loader)\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (token_data, mask_pos_data, mask_token_data, code0, code1, code2, padding_mask) in enumerate(val_loader):\n",
    "            # remove the extra dimension\n",
    "            token_data, mask_pos_data, mask_token_data, padding_mask, code0, code1, code2 = token_data.squeeze(1), \\\n",
    "            mask_pos_data.squeeze(1), mask_token_data.squeeze(1), padding_mask.squeeze(1),\\\n",
    "                code0.squeeze(1), code1.squeeze(1), code2.squeeze(1)\n",
    "\n",
    "            MaskedLM, code0_pred, code1_pred, code2_pred = Bert_model(input=token_data, padding_mask=padding_mask)\n",
    "            # masked token prediction\n",
    "            # (batch_size, n_mask) --> (batch_size,n_mask,vocab_size)\n",
    "            masked_pos = mask_pos_data.unsqueeze(-1).expand(-1, -1, MaskedLM.size(-1))\n",
    "            # (batch_size,source_len,vocab_size) --> (batch_size,n_mask,vocab_size)\n",
    "            MaskedLM = torch.gather(MaskedLM, 1, masked_pos)\n",
    "            # calculate the training loss       \n",
    "            loss_maskLM = Loss(MaskedLM.transpose(1,2), mask_token_data)\n",
    "            loss_code0 = Loss(code0_pred, code0.float())\n",
    "            curr_loss = loss_maskLM + loss_code0\n",
    "            val_loss += curr_loss.item()\n",
    "            \n",
    "        epoch_val_loss = val_loss/len(val_loader)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}, training loss: {epoch_train_loss:.5f}, validation loss: {epoch_val_loss:.5f}\")\n",
    "        \n",
    "        if epoch_val_loss < min_val_loss:\n",
    "            if verbose:\n",
    "                print(f'Validation loss decreased: ({min_val_loss:.5f}-->{epoch_val_loss:.5f}), saving the model.')\n",
    "            min_val_loss = epoch_val_loss\n",
    "            \n",
    "            # save the model\n",
    "            torch.save(Bert_model.state_dict(), 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "plt.plot(train_loss_history, color=\"blue\")\n",
    "plt.plot(val_loss_history, color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/training-neural-networks-with-validation-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for modelling?\n",
    "# why val loss lower than the val loss?\n",
    "# why training loss is different after each run? --> need to control the inner seed in pytorch\n",
    "# for code1/2, model it as a classification question! for rare types, combine all of them as others, or delete them?\n",
    "# search for imbalanced classification?\n",
    "\n",
    "# calculate the accuracy, matrix\n",
    "# do CV under each epoch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for course report:\n",
    "\n",
    "# # create another repo, delete unnecessay folder, write down readme, add a mini training set\n",
    "# need to rewrite part of the bert\n",
    "# convert notebook to .py\n",
    "# how to make the code neater?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = []\n",
    "# Bert_model = bert.Bert(ebd_dim=ebd_dim, num_head=num_head, vocab_size=vocab_size,\n",
    "#                         dim_feedforward=dim_feedforward,\n",
    "#                                 num_layer=num_layer, max_seq_len=max_seq_len)\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     dataset = preprocess.seq_dataset(data_path=os.path.join(os.path.dirname(curr_path), \n",
    "#                                             \"data/imgt_I_domain_1_2.csv\"), seqs_range = train_val_idx,\n",
    "#                                           seed=epoch)\n",
    "#     train_set_size = int(len(dataset) * 0.8)\n",
    "#     val_set_size = len(dataset) - train_set_size\n",
    "#     train_set, val_set = torch.utils.data.random_split(dataset, [train_set_size, val_set_size])\n",
    "#     train_loader = DataLoader(dataset=train_set, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "#     Loss = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(Bert_model.parameters(), lr=lr)\n",
    "    \n",
    "#     for i, (token_data, mask_pos_data, mask_token_data, code0, code1, code2, pad_mask) in enumerate(train_loader):\n",
    "        \n",
    "#         # remove the extra dimension\n",
    "#         token_data, mask_pos_data, mask_token_data, padding_mask = token_data.squeeze(1), \\\n",
    "#         mask_pos_data.squeeze(1), mask_token_data.squeeze(1), padding_mask.squeeze(1)\n",
    "#         # train the model\n",
    "#         optimizer.zero_grad()\n",
    "#         NSP, MaskedLM = Bert_model(input=token_data, padding_mask=padding_mask)\n",
    "        \n",
    "#         # masked token prediction\n",
    "#         # (batch_size, n_mask) --> (batch_size,n_mask,ebd_dim)\n",
    "#         masked_pos = mask_pos_data.unsqueeze(-1).expand(-1, -1, MaskedLM.size(-1))\n",
    "#         # (batch_size,source_len,ebd_dim) --> (batch_size,n_mask,ebd_dim)\n",
    "#         MaskedLM = torch.gather(MaskedLM, 1, masked_pos)\n",
    "        \n",
    "#         # NSP predictions\n",
    "        \n",
    "        \n",
    "#         # calculate the loss\n",
    "#         loss_maskLM = Loss(MaskedLM.transpose(1,2), mask_token_data)\n",
    "#         loss = (loss_maskLM.float()).mean()\n",
    "        \n",
    "#         # protein type prediction\n",
    "        \n",
    "        \n",
    "#         # if (i+1) % 10 == 0:\n",
    "#         print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{num_iters}, cost = {loss}')\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     # record loss in each epoch\n",
    "#     loss_history.append(loss.item())\n",
    "        \n",
    "# # plot the loss\n",
    "# plt.plot(loss_history)\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Training Loss\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df25935b99ec2a4cd8586e5ce280559874741e178bd8136094667422bad848bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
